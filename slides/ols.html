<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>Ordinary least squares regression</title>
    <meta charset="utf-8" />
    <meta name="author" content="Jüri Lillemets" />
    <meta name="date" content="2019-09-22" />
    <link rel="stylesheet" href="minimal.css" type="text/css" />
    <link rel="stylesheet" href="fonts.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Ordinary least squares regression
## Data and statistics with R <br> Research Methods <br><br>
### Jüri Lillemets
### 2019-09-22

---

  


---

class: center middle inverse

# Ordinary least squares regression

---

# Example problem

Weight gain calves in a feedlot, given three different diets.


```r
calves &lt;- agridat::urquhart.feedlot
head(calves, 3)
```

```
##   animal herd diet weight1 weight2
## 1      1    9  Low     575     826
## 2      2    9  Low     605     816
## 3      3    9  Low     640     902
```

- `animal`, animal ID
- `herd`, herd ID
- `diet`, diet: Low, Medium, High
- `weight1`, initial weight
- `weight2`, slaughter weight

---

# Example problem

.pull-left[
Initial and slaughter weight of calves.
![](ols_files/figure-html/unnamed-chunk-2-1.png)&lt;!-- --&gt;
]

--

.pull-right[
Does 1st weight have an effect on 2nd weight?

How much of 2nd weight depends on 1st weight?

How much does 2nd weight increase when 1st weight increases?

What could be the 2nd weight when 1st weight is e.g. 700?

Can we quantify this relationship?
]

---

We can model this using regression!

![](ols_files/figure-html/unnamed-chunk-3-1.png)&lt;!-- --&gt;

---

# Ordinarly least squares

Also called OLS.

Simplest regression model.

Works only for relationships that are linear (in parameters).

---

# Model specification

Simple linear regression model has one predictor.

`$$Y = \alpha + \beta x + \varepsilon,$$`

where

- `\(Y\)` is dependent or explained variable, **response** or regressand, 
- `\(\alpha\)` is the intercept or constant, 
- `\(\beta\)` is a coefficient, 
- `\(x\)` is independent or explanatory variable, **predictor** or regressor, and
- `\(\varepsilon\)` is the model error.

---

# Calculation

Minimization of (squared) residuals.

Model parameters are calculated (unlike maximum likelihood estimation based on iterations). 

To estimate the model `\(Y = \alpha + \beta x + \varepsilon\)` :

`\(\hat{\beta} = \frac{\sum{x_{i} y_{i}} - \frac{1}{n} \sum{x_{i}}\sum{y_{i}}}{\sum{x_{i}^{2}} - \frac{1}{n} (\sum{x_{i}})^{2}} = \frac{Cov[x,y]} {Var[x]}\)`

`\(\hat{\alpha} = \overline{y} - \beta \overline{x}\)`

---

For a model `\(y = \beta x + \varepsilon\)` we can simply use matrix algebra on values of `\(x\)` and `\(y\)`:

`$$\hat{\beta} = (X^{T} X)^{-1} X^{T} Y$$`

In our example problem `\(X\)` would be the vector of 1st weights.


```r
calves$weight1
```

```
##  [1] 575 605 640 600 610 575 730 670 690 685 690 670 755 655 725 750 705
## [18] 785 685 655 750 715 785 795 640 680 620 765 750 645 730 795 605 570
## [35] 730 670 700 665 635 700 715 675 770 800 685 715 865 845 705 725 840
## [52] 755 725 770 755 530 680 605 665 765 720 780 675 705 755 755 750
```

And `\(Y\)` would be the vector of 2nd weights.


```r
calves$weight1
```

```
##  [1] 575 605 640 600 610 575 730 670 690 685 690 670 755 655 725 750 705
## [18] 785 685 655 750 715 785 795 640 680 620 765 750 645 730 795 605 570
## [35] 730 670 700 665 635 700 715 675 770 800 685 715 865 845 705 725 840
## [52] 755 725 770 755 530 680 605 665 765 720 780 675 705 755 755 750
```

---

# Estimating an OLS model in R

.pull-left[
Function `lm()` (linear model).

The model is defined as a formula.

`\(Y = \alpha + \beta x + \varepsilon\)`

In R this is expressed as:  
`y ~ x`  
or  
`y ~ x + z`
]

.pull-right[

```r
wtMod &lt;- lm(weight2 ~ weight1, calves)
summary(wtMod)
```

```
## 
## Call:
## lm(formula = weight2 ~ weight1, data = calves)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -190.66  -32.48   11.59   34.30  118.13 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 195.1352    76.5219    2.55   0.0131 *  
## weight1       1.1758     0.1083   10.86 3.02e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 61.27 on 65 degrees of freedom
## Multiple R-squared:  0.6447,	Adjusted R-squared:  0.6392 
## F-statistic: 117.9 on 1 and 65 DF,  p-value: 3.017e-16
```
]

---

class: center middle inverse

# Some elements of OLS models

---

# Intercept

.pull-left[
In the formula the `\(\alpha\)`.Sometimes called constant. The value of `\(y\)` where the value of `\(x\)` is zero ( `\(y|x=0\)` ).

In our example 2nd weight is 195 if 1st weight is 0. Intercept not need to be theoretically valid but it sometimes is.
]

.pull-right[
![](ols_files/figure-html/unnamed-chunk-7-1.png)&lt;!-- --&gt;
]

---

# Coefficient(s)

.pull-left[
In formula the `\(\beta\)`. Indicates how much y increases when `\(x\)` increases. In our example every kg of 1st weight increases 2nd weight by 1.176 kg (in the plot *100).

Coefficients are only relevant when their difference from 0 is statistically significant.
]

.pull-right[
![](ols_files/figure-html/unnamed-chunk-8-1.png)&lt;!-- --&gt;
]

---

We can not be sure if the coefficients are actually significant, especially when estimation is done on a sample. 

It is thus necessary to test whether or not coefficients are different from 0. This is done by calculating t-statistic from coefficient and standard error.

---

# Residuals

.pull-left[
In formula the `\(\varepsilon\)`. Residuals are the difference of response between observed and fitted values

We use residuals to evaluate how well model fits data. If residuals are large, the model is not very good.
]

.pull-right[
![](ols_files/figure-html/unnamed-chunk-9-1.png)&lt;!-- --&gt;
]

---

# Fitted values

.pull-left[
These are the values of `\(y\)` calculated using the model for every `\(x\)` in the data. 

In other words, predictions.
]

.pull-right[
![](ols_files/figure-html/unnamed-chunk-10-1.png)&lt;!-- --&gt;
]

---

# The `\(R^2\)` 

A *goodness of fit* measure: measures how well model fits data. Indicates the **part of variation in response variable that is explained by the model**:

`$$R^{2} = \frac{ESS}{TSS} = 1 - \frac{RSS}{TSS}$$`

- **e**xplained sum of squares, `\(ESS\)`
  `\(\sum_{i = 1}^{n} (\hat{y}_{i} - \overline{y})^2\)`
- **r**esidual sum of squares, `\(RSS\)`
  `\(\sum_{i = 1}^{n} (y_{i} - \hat{y}_{i})^2\)`
- **t**otal sum of squares, `\(TSS\)`
  `\(\sum_{i = 1}^{n} (y_{i} - \overline{y}_{i})^2\)`

---

How much better is model at explaining the variance of `\(y\)` compared to just the mean?

![](ols_files/figure-html/unnamed-chunk-11-1.png)&lt;!-- --&gt;

---

# The adjusted- `\(R^2\)` 

The more variables we add, the more the model explains.

To penalize a model for the number of predictors ( `\(K\)` ) while considering the number of observations ( `\(N\)` ), the adjusted `\(R^{2}\)` can also be used, particularly for model comparison:

`$$\overline{R^{2}} = 1 - \frac{RSS/(N-K)}{TSS/(N-K)}$$`

---

# These are in the output of  `summary.lm()`


```r
summary(wtMod)
```

```
## 
## Call:
## lm(formula = weight2 ~ weight1, data = calves)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -190.66  -32.48   11.59   34.30  118.13 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 195.1352    76.5219    2.55   0.0131 *  
## weight1       1.1758     0.1083   10.86 3.02e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 61.27 on 65 degrees of freedom
## Multiple R-squared:  0.6447,	Adjusted R-squared:  0.6392 
## F-statistic: 117.9 on 1 and 65 DF,  p-value: 3.017e-16
```

---

How are the parameters related to the formula?

`$$weight2_{i} = 195 + 1.176 * weight1_{i} + \varepsilon_{i}$$`


```r
calves$weight2[1]
```

```
## [1] 826
```

```r
wtMod$coef[[1]] + wtMod$coef[[2]] * calves$weight1[[1]] + wtMod$resid[[1]]
```

```
## [1] 826
```

---

class: center middle inverse

# Assumptions and diagnostics

---

# Residuals are normally distributed

.pull-left[

```r
hist(wtMod$residuals, 30)
```

![](ols_files/figure-html/unnamed-chunk-14-1.png)&lt;!-- --&gt;
]

.pull-right[

```r
plot(wtMod, 2)
```

![](ols_files/figure-html/unnamed-chunk-15-1.png)&lt;!-- --&gt;
]

---

# Residuals have equal variance


```r
plot(wtMod, 1)
```

![](ols_files/figure-html/unnamed-chunk-16-1.png)&lt;!-- --&gt;

Here the we have equal variance, e.g. homoscedasticity.

---

Example of heteroscedasticity. Here higher values have higher variance than lower.

![](ols_files/figure-html/unnamed-chunk-17-1.png)&lt;!-- --&gt;

---

# There is no multicollinearity

May be problem when model has multiple predictors.

It should not be possible to linearly predict any of the predictors from others predictors. Otherwise the coefficients will not be reliable.

This can be detected with variance inflation factor by using `\(Rpredictor._{2}\)` to estimate how much of the variation in one predictor can be predicted from others.

`$$VIF_{i} = \frac{1}{1 - R^{2}_{i}}$$`

---

# Gauss-Markov assumptions

Linear model must fulfill the following conditions.

- linear in parameters  
  `\(Y = \alpha + \beta x + \varepsilon\)`
- expected error is zero  
  `\(E(\varepsilon) = 0\)`
- homoscedasticity  
  `\(var(\varepsilon) = E(\varepsilon^{2})\)`
- no autocorrelation  
  `\(cov(\varepsilon_{i}, \varepsilon_{j}) = 0, i \neq j\)`
- independence of predictor(s) and residuals  
  `\(cov(x,\varepsilon) = 0\)`
  
If these are true, then the model is the *best linear unbiased estimator* (BLUE).

---

class: center middle inverse 

# Improving the functional form

---

# Use logs

When the distribution is highly skewed, we may wish to use (natural) logged values of variable(s). 

We can specify this simply in the model definition, e.g. `log(weight2) ~ weight1`

---

# Log-linear model

.pull-left[

```r
wtModLogLin &lt;- lm(log(weight2) ~ weight1, calves)
summary(wtModLogLin)
```

```
## 
## Call:
## lm(formula = log(weight2) ~ weight1, data = calves)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.18391 -0.03593  0.01652  0.03368  0.10233 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 6.1051966  0.0761147   80.21  &lt; 2e-16 ***
## weight1     0.0011650  0.0001077   10.82 3.55e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.06094 on 65 degrees of freedom
## Multiple R-squared:  0.643,	Adjusted R-squared:  0.6375 
## F-statistic:   117 on 1 and 65 DF,  p-value: 3.546e-16
```
]

.pull-right[
When 1st weight increases by 1 kg, 2nd weight increases by 0.117 percent.
]

---


```r
par(mfrow = c(1,2))
plot(wtMod, 1)
plot(wtModLogLin, 1)
```

![](ols_files/figure-html/unnamed-chunk-19-1.png)&lt;!-- --&gt;

---

# Log-log model

.pull-left[

```r
wtModLogLog &lt;- lm(log(weight2) ~ log(weight1), calves)
summary(wtModLogLog)
```

```
## 
## Call:
## lm(formula = log(weight2) ~ log(weight1), data = calves)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.17089 -0.03402  0.01244  0.03604  0.10186 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   1.63949    0.48533   3.378  0.00124 ** 
## log(weight1)  0.80678    0.07408  10.891 2.68e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.06068 on 65 degrees of freedom
## Multiple R-squared:  0.646,	Adjusted R-squared:  0.6406 
## F-statistic: 118.6 on 1 and 65 DF,  p-value: 2.676e-16
```
]

.pull-right[
When 1st weight increases by 1 percent, 2nd weight increases by 0.807 percent.
]

---

# Use polynomials

Response `\(Y\)` can be modelled as some degree polynomial in predictor `\(X\)` . This means adding the predictor as squared, cubic, quadratic or higher term.

In the example the formula of square term added would be `weight2 ~ weight1 + I(weight1^2)`. We simply add variable `weight1` as squared and make R treat it *as is* by enclosing it in `I()`.

What this means: `\(Y = \alpha + \beta_{1} x  + \beta_{2} x^2+ \varepsilon\)`

---

.pull-left[

```r
wtModSq &lt;- lm(weight2 ~ weight1 + I(weight1^2), calves)
summary(wtModSq)
```

```
## 
## Call:
## lm(formula = weight2 ~ weight1 + I(weight1^2), data = calves)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -180.34  -29.98   11.94   34.18  120.90 
## 
## Coefficients:
##                Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)  -1.061e+02  5.762e+02  -0.184    0.854
## weight1       2.047e+00  1.654e+00   1.237    0.221
## I(weight1^2) -6.230e-04  1.181e-03  -0.528    0.600
## 
## Residual standard error: 61.61 on 64 degrees of freedom
## Multiple R-squared:  0.6462,	Adjusted R-squared:  0.6352 
## F-statistic: 58.46 on 2 and 64 DF,  p-value: 3.617e-15
```
]

.pull-right[
Did the R-squared improve?

```r
summary(wtMod)$r.squared
```

```
## [1] 0.6447088
```

```r
summary(wtModSq)$r.squared
```

```
## [1] 0.6462473
```
We can't really interpret the coefficients now.
]

---

We can use `anova()` to test wether or not the squared term improves model.


```r
anova(wtMod, wtModLogLin)
```

```
## Warning in anova.lmlist(object, ...): models with response '"log(weight2)"'
## removed because response differs from model 1
```

```
## Analysis of Variance Table
## 
## Response: weight2
##           Df Sum Sq Mean Sq F value    Pr(&gt;F)    
## weight1    1 442738  442738  117.95 3.017e-16 ***
## Residuals 65 243987    3754                      
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
```

Here the p-value of 0.6 indicates that model with squared term is not significantly different (better).

---

class: center middle inverse

# Multivariate models

---

# Let's add a predictor

Does diet also have an effect on 2nd weight? 

![](ols_files/figure-html/unnamed-chunk-24-1.png)&lt;!-- --&gt;

---

We can add multiple predictors to model and these can even be factors. Because all factor levels are evaluated against the first level, we may want to change it.


```r
levels(calves$diet)
```

```
## [1] "High"   "Low"    "Medium"
```

```r
calves$diet &lt;- factor(calves$diet, levels = c('Low', 'Medium', 'High'))
```

---

.pull-left[

```r
wtMod &lt;- lm(weight2 ~ weight1 + diet, calves)
summary(wtMod)
```

```
## 
## Call:
## lm(formula = weight2 ~ weight1 + diet, data = calves)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -173.402  -34.832    9.578   37.038  101.549 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 171.4802    75.1346   2.282   0.0259 *  
## weight1       1.2264     0.1076  11.398   &lt;2e-16 ***
## dietMedium  -36.3679    16.6368  -2.186   0.0325 *  
## dietHigh      3.0042    20.1926   0.149   0.8822    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 59.61 on 63 degrees of freedom
## Multiple R-squared:  0.674,	Adjusted R-squared:  0.6585 
## F-statistic: 43.43 on 3 and 63 DF,  p-value: 2.447e-15
```
]

.pull-right[
The coefficient for 1st weight slightly increased. 

The 2nd weight if 1st weight is 0 and diet is low is 171.48.

All the values for variable `diet` are compared to base level, i.e. *Low*. 

Medium diet actually decreases 2nd weight by  -36.368 compared to *low* diet. 

*High* diet is no different from *Low* as the coefficient is not statistically significant.
]

---

# Factors in OLS

.pull-left[

```r
summary(lm(weight2 ~ diet, calves))
```

```
## 
## Call:
## lm(formula = weight2 ~ diet, data = calves)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -203.406  -61.361    7.594   82.583  228.594 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 1019.406     18.294  55.722   &lt;2e-16 ***
## dietMedium     1.898     28.290   0.067    0.947    
## dietHigh      12.010     35.031   0.343    0.733    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 103.5 on 64 degrees of freedom
## Multiple R-squared:  0.001875,	Adjusted R-squared:  -0.02932 
## F-statistic: 0.06011 on 2 and 64 DF,  p-value: 0.9417
```
]

.pull-right[

```r
aggregate(weight2 ~ diet, calves, mean)
```

```
##     diet  weight2
## 1    Low 1019.406
## 2 Medium 1021.304
## 3   High 1031.417
```

Anova is equivalent to OLS with one or more factor variables.


```r
summary(aov(weight2 ~ diet, calves))
```

```
##             Df Sum Sq Mean Sq F value Pr(&gt;F)
## diet         2   1288     644    0.06  0.942
## Residuals   64 685438   10710
```
]

---

# Why use multiple predictors?

.pull-left[

```r
lm(weight2 ~ weight1, calves)$coef
```

```
## (Intercept)     weight1 
##  195.135237    1.175768
```


```r
lm(weight2 ~ weight1 + diet, calves)$coef
```

```
## (Intercept)     weight1  dietMedium    dietHigh 
##  171.480247    1.226379  -36.367922    3.004197
```
]

--

.pull-right[
Treating only diet as a predictor would have misled us.

```r
lm(weight2 ~ diet, calves)$coef
```

```
## (Intercept)  dietMedium    dietHigh 
## 1019.406250    1.898098   12.010417
```


```r
aggregate(cbind(weight1, weight2) ~ diet, calves, mean)
```

```
##     diet  weight1  weight2
## 1    Low 691.4062 1019.406
## 2 Medium 722.6087 1021.304
## 3   High 698.7500 1031.417
```
]

---

class: inverse
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"ratio": "16:9",
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
